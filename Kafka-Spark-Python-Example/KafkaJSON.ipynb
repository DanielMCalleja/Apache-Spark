{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming - Kafka JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0)Abrimos una terminal. Arrancamos zookeeper y kafka en dos pestañas por separado.\n",
    "**$ zookeeper-server-start.sh config/zookeeper.properties\n",
    "$ kafka-server-start.sh config/server.properties**\n",
    "\n",
    "Creamos el topic *json_topic* en una nueva terminal.\n",
    "**$ kafka-topics.sh --zookeeper 127.0.0.1:2181 --topic json_topic --create --partitions 3 --replication-factor 1**\n",
    "\n",
    "A continuación, creamos el producer.\n",
    "**$ kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic json_topic**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)El primer paso es crear una sesión de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PEC2StructuredKafkaWordCount\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)A continuación, definimos un input stream. \n",
    "  En las opciones especificamos el nombre del topic **json_topic**, el cual se ha creado previamente en otra ventana.\n",
    "  También especificamos el host:port del productor con **kafka.bootstrap.servers**\n",
    "  Con la opción value.deserializer deserializamos los valores. Los valores son siempre deserializados como un array\n",
    "  de bytes con ByteArrayDeserializer.\n",
    "  Con la opción **auto.offset.reset** especificamos donde comenzar. **Structured Streaming** es capaz de \n",
    "  gestionar cuales offsets son consumidos en lugar de basarse en el consumidor de kafka.\n",
    "  Asegurándose que no se pierden datos cuando un nuevo topic es subscrito. Con **earliest** aplica desde el principio cuando una nueva query comienza.\n",
    "  Con la función printSchema mostramos el esquema del **stream dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputStream = spark\\\n",
    "        .readStream\\\n",
    "        .format(\"kafka\")\\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "        .option(\"auto.offset.reset\",\"earliest\") \\\n",
    "        .option(\"value.deserializer\", \"StringDeserializer\")\\\n",
    "        .option(\"subscribe\", \"json_topic\")\\\n",
    "        .option('includeTimestamp', 'true') \\\n",
    "        .load()\n",
    "\n",
    "inputStream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)Definimos el esquema json, el cual vamos a utilizar para el procesamiento de los json que vamos a mandar a través del consumidor. Los registros leidos desde el topic de Kafka tiene estructura JSON. Por lo que necesitamos convertir nuestro valor de cadena del topic de Kafka, el cual tiene un tipo binario en una estructura significativa, pasándolo a Dataframe. En este caso seleccionamos **value** y **timestamp**, el primero contendrá los valores de los campos de la estructura que se van pasando a traves del kafka stream. Por otro lado, también pasamos timestamp del stream dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType() \\\n",
    "        .add(\"nombre\", StringType()) \\\n",
    "        .add(\"edad\", IntegerType()) \\\n",
    "        .add(\"peso\", FloatType()) \n",
    "        \n",
    "    \n",
    "initial = inputStream.selectExpr(\"CAST(value AS STRING)\",\"timestamp as timestamp\").toDF(\"value\",\"timestamp\")\n",
    "initial.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)La siguiente línea de comandos es para convertir **value** en su representación JSON. Con esta expresión, la cadena de entrada está siendo deserializada en su valor JSON. El último select **.select(\"parsed_value.*\",\"time\")** de la expresión se utiliza para seleccionar el contenido embebido de la estructura anidada que contiene todos los campos JSON.\n",
    "Utilizamos la función **printSchema** para ver como queda el esquema que tenemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- edad: integer (nullable = true)\n",
      " |-- peso: float (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggregation = initial.select(from_json(col(\"value\"), schema).alias(\"parsed_value\"),col(\"timestamp\").alias(\"time\"))\\\n",
    "    .select(\"parsed_value.*\",\"time\")\n",
    "    \n",
    "aggregation.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)A continuación, definimos un tamaño de ventana y una marca de agua. La ventana tiene 3 parámetros.\n",
    "- timeColumn: en el que hemos indicado un tamaño de ventana, basándonos en el timestamp de kafka.\n",
    "- windowDuration: el tamaño de ventana que usamos es de 10 minutos.\n",
    "- slideDuration: el deslizamiento de ventana que indicamos es de 5 minutos.\n",
    "\n",
    "Utilizamos la función **GroupBy** para agrupar los datos en base a la ventana de tiempo y el nombre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f3ab8192240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " windowedCounts = aggregation \\\n",
    "    .withWatermark(\"time\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"time\"), \"10 minutes\", \"5 minutes\"),\n",
    "        aggregation.nombre) \n",
    "display(windowedCounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6)En la siguiente línea, ejecutamos una operación de aggregation.Recordemos que en el paso anterior ejecutamos una función de agrupacion, por lo que tenemos un Dataframe con las columnas agrupadas. Con la operación agg añadimos columnas adicionales, las cuales contienen operaciones como promedios, o calculo de máximo o mínimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregatedDF = windowedCounts.agg(avg(\"edad\").alias(\"media_edad\"), avg(\"peso\").alias(\"media_peso\"),\\\n",
    "                                  max(\"peso\").alias(\"peso_max\"),min(\"peso\").alias(\"peso_min\"),\\\n",
    "                                  max(\"edad\").alias(\"edad_max\"),min(\"edad\").alias(\"edad_min\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6)El paso final, es escribir los datos agregados en un **sink** o fregadero. En nuestro caso, el fregadero usado es la consola, pero se podría haber utilizado Hive, u otro tipo de formato. El outputMode elegido es **complete**, de esta manera todos los resultados de la tabla son enviados al fregadero después de cada procesamiento.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = aggregatedDF.writeStream \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option('truncate', 'false') \\\n",
    "        .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
